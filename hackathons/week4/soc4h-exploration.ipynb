{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Steps\n",
    "\n",
    "If the data is huge, you may want to sample smaller training sets so you can train many different\n",
    "models in a reasonable time (be aware that this penalizes complex models such as large neural nets\n",
    "or Random Forests).\n",
    "Once again, try to automate these steps as much as possible.\n",
    "\n",
    "1. Train many quick and dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "2. Measure and compare their performance. For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "3. Analyze the most significant variables for each algorithm.\n",
    "4. Analyze the types of errors the models make. What data would a human have used to avoid these errors?\n",
    "5. Have a quick round of feature selection and engineering.\n",
    "6. Have one or two more quick iterations of the five previous steps.\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors.\n",
    "\n",
    "Source: p. 646. Hands-on Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis options\n",
    "\n",
    "### Features\n",
    "\n",
    "- Length of review\n",
    "\n",
    "*Ignoring word order*\n",
    "- Frequencies of all relevant words (ignore stop words)\n",
    "OR\n",
    "- Frequency of positive words (list from NLTK)\n",
    "- Frequency of negative words (list from NLTK), etc.\n",
    "\n",
    "*Keeping word order*\n",
    "- One hot encoding of words -> list of all words in vocab (as a vector/column) with row respective to word = 1\n",
    "- Word embeddings -> features of word learned through different algorithms\n",
    "Video on word embeddings; https://www.youtube.com/watch?v=186HUTBQnpY\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "#### Non-sequential algorithms\n",
    "\n",
    "Any classification algorithms;\n",
    "- Logistic Regression\n",
    "- Decision trees\n",
    "- Naive Bayes\n",
    "\n",
    "#### Sequential algorithms\n",
    "\n",
    "RNNs (~ BRNNs, LSTMs, GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Covered In Class\n",
    "\n",
    "### Linear Regression - Baseline Model\n",
    "Features; number of +ve words, Number of -ve words\n",
    "\n",
    "The mean absolute error on the training data is 0.832466 stars\n",
    "\n",
    "### Random Forests (non-linear model)\n",
    "Features; number of +ve words, Number of -ve words -> after taking into account negations, e.g. (not good)\n",
    "\n",
    "A nonlinear regressor achieves a MAE of 0.715708 stars\n",
    "\n",
    "### Linear Regression with NLTK Sentiment Intensity Analyser\n",
    "Features; number of +ve words, Number of -ve words -> after taking into account negations e.g. (not good) & *(see below)\n",
    "\n",
    "Now the mean absolute error on the training data is 0.758256 stars\n",
    "\n",
    "On the validation set, we get 0.755795 error for the linear regression\n",
    "\n",
    "### Random Forests with NLTK Sentiment Intensity Analyser\n",
    "Features; number of +ve words, Number of -ve words -> after taking into account negations e.g. (not good) & *(see below)\n",
    "\n",
    "For the RF, it is 0.283528 stars\n",
    "\n",
    "Validation set; 0.731631 for the random forest regression\n",
    "\n",
    "* *Features*\n",
    "\n",
    "       (1) the mean positive sentiment over all sentences\n",
    "       (2) the mean neutral sentiment over all sentences\n",
    "       (3) the mean negative sentiment over all sentences\n",
    "       (4) the maximum positive sentiment over all sentences\n",
    "       (5) the maximum neutral sentiment over all sentences\n",
    "       (6) the maximum negative sentiment over all sentences\n",
    "       (7) length of review (in thousands of characters) - truncate at 2,500\n",
    "       (8) percentage of exclamation marks (in %)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "- Naive Bayes classifier\n",
    "\n",
    "https://www.youtube.com/watch?v=tOP5DzKxc20\n",
    "\n",
    "https://www.youtube.com/watch?v=5YymjfzMpL8\n",
    "\n",
    "Choices; Negate words?\n",
    "\n",
    "- RNNs (BRNNs?) with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
